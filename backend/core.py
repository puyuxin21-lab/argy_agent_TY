import os
# âœ… æ­£ç¡®çš„å†™æ³•
from langchain_community.document_loaders import DirectoryLoader, TextLoader,PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from dotenv import load_dotenv, find_dotenv

# --- é…ç½® ---
#os.environ["OPENAI_API_KEY"] = "sk-i0HXYWyGQZ6v5VKdoM0alDBvTpPD8GxVHja1ex6rR0lfP29G"
#os.environ["OPENAI_API_BASE"] = "https://api.openai-proxy.org/v1"
#os.environ["HF_ENDPOINT"] = "https://hf-mirror.com" # å›½å†…é•œåƒ
#API_KEY=os.getenv("OPENAI_API_KEY")
#API_BASE=os.getenv("OPENAI_API_BASE")
load_dotenv(find_dotenv(),override=True)
HF_ENDPOINT=os.getenv("HF_ENDPOINT")

INDEX_PATH = "faiss_index_store"
DATA_PATH = "../data"#æ³¨æ„ï¼šç›¸å¯¹äº backendæ–‡ä»¶å¤¹ï¼Œdataåœ¨ä¸Šä¸€çº§
LOCAL_MODEL_NAME = os.getenv("LOCAL_MODEL_NAME")

class AllergyAgentAI:
    def __init__(self):
        """åˆå§‹åŒ–"""
        #é»˜è®¤LLMé…ç½®ï¼ˆå­˜æ”¾åœ¨å†…å­˜ä¸­ï¼‰
        self.current_llm_model = "gpt-3.5-turbo"
        self.current_llm_temperature = 0.2

        """åˆå§‹åŒ–æ—¶åŠ è½½æœ¬åœ°æ¨¡å‹"""
        print(f"[AI Core] æ­£åœ¨åŠ è½½æœ¬åœ°æ¨¡å‹: {LOCAL_MODEL_NAME} ...")
        try:
            self.embeddings = HuggingFaceEmbeddings(
            model_name=LOCAL_MODEL_NAME,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
            )
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.embeddings = None
        self.vectorstore = None
        self.load_vector_store()#å°è¯•åŠ è½½å·²æœ‰ç´¢å¼•

    #æ›´æ–°é…ç½®çš„æ–¹æ³•
    def update_llm_config(self,model: str = None,temperature: float = None):
        if model:
            self.current_llm_model = model
        if temperature is not None:
            #é™åˆ¶æ¸©åº¦èŒƒå›´ 0.0 - 2.0
            self.current_llm_temperature = max(0.0,min(2.0,temperature))
        print(f"[Config] é…ç½®å·²æ›´æ–°ï¼šModel={self.current_llm_model},Temp={self.current_llm_temperature}")
        return self.get_llm_config()

    #è·å–å½“å‰é…ç½®
    def get_llm_config(self):
        return {
            "model":self.current_llm_model,
            "temperature":self.current_llm_temperature
        }

    def load_vector_store(self):
        """å°è¯•ä»ç¡¬ç›˜åŠ è½½ç´¢å¼•"""
        if os.path.exists(INDEX_PATH):
            try:
                self.vectorstore = FAISS.load_local(
                    INDEX_PATH,
                    self.embeddings,
                    allow_dangerous_deserialization=True
                )
                print("[AI Core] æˆåŠŸåŠ è½½æœ¬åœ°çŸ¥è¯†åº“ç´¢å¼•")
            except Exception as e:
                print("[AI Core] åŠ è½½ç´¢å¼•å¤±è´¥: {e}")
        else:
            print("[AI Core] æœªæ‰¾åˆ°æœ¬åœ°ç´¢å¼•ï¼Œè¯·å…ˆæ‰§è¡Œé‡å»ºçŸ¥è¯†åº“")

    def rebuild_knowledge_base(self):
        """é‡å»ºçŸ¥è¯†åº“(ETL æµç¨‹ æ”¯æŒTXTå’ŒPDF)"""
        print(f"[AI Core] é‡å»ºä¸­... è¯»å–ç›®å½•ï¼š{os.path.abspath(DATA_PATH)}")

        if not self.embeddings:
            return {"status":"error","message":"æ¨¡å‹æœªåŠ è½½"}

        if not os.path.exists(DATA_PATH):
            try:
                os.makedirs(DATA_PATH)
            except:
                return {"status":"error","message":"åˆ›å»ºdataæ–‡ä»¶å¤¹å¤±è´¥"}

        docs = []

        #éå†ç›®å½•ï¼ŒåŒºåˆ†å¤„ç†ä¸åŒæ ¼å¼çš„æ–‡ä»¶
        try:
            for filename in os.listdir(DATA_PATH):
                file_path = os.path.join(DATA_PATH, filename)

                if filename.lower().endswith(".txt"):
                    #åŠ è½½TXT
                    loader = TextLoader(file_path,encoding="utf-8")
                    docs.extend(loader.load())

                elif filename.lower().endswith(".pdf"):
                    #åŠ è½½PDF
                    loader = PyPDFLoader(file_path)
                    docs.extend(loader.load())
        except Exception as e:
            return {"status":"error","message":f"æ–‡ä»¶è¯»å–å¤±è´¥{e}"}

        if not docs:
            return {"status":"warning","message":"data ç›®å½•æ²¡æœ‰å¯è¯†åˆ«çš„æ–‡ä»¶ ( .txt/.pdf)"}

        #2.åˆ‡åˆ† ï¼ˆpdfé€šå¸¸å†…å®¹è¾ƒå¤šï¼ŒChunk Size ä¿æŒ400æ¯”è¾ƒåˆé€‚ï¼‰
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=400,chunk_overlap=50)
        splits = text_splitter.split_documents(docs)


        #3.å‘é‡åŒ–å¹¶ä¿å­˜
        try:
            self.vectorstore = FAISS.from_documents(splits,self.embeddings)
            self.vectorstore.save_local(INDEX_PATH)
            return {"status":"success","message":f"çŸ¥è¯†åº“æ„å»ºæˆåŠŸï¼Œæ”¶å½•{len(splits)}æ¡ç‰‡æ®µ"}
        except Exception as e:
            return {"status":"error","message":str(e)}

    def chat(self,question: str):
        """é—®ç­”æ ¸å¿ƒé€»è¾‘"""
        if not self.vectorstore:
            return "çŸ¥è¯†åº“å°šæœªå»ºç«‹ï¼Œè¯·è”ç³»ç®¡ç†å‘˜é‡å»ºçŸ¥è¯†åº“ã€‚"

        #æ£€ç´¢
        retriever = self.vectorstore.as_retriever(search_kwargs={"k": 3})

        #ç”Ÿæˆ
        llm = ChatOpenAI(model="gpt-3.5-turbo",temperature=0.1)

        template = """
                ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼ä¸”ä¸“ä¸šçš„çŸ¥è¯†åº“é—®ç­”åŠ©æ‰‹ï¼Œåå«â€œæ•å®å®ˆæŠ¤è€…â€ã€‚
            
                ğŸ”´ ä¸¥ç¦äº‹é¡¹ï¼š
                1. **ä¸¥ç¦**ä½¿ç”¨ä½ çš„è®­ç»ƒæ•°æ®ï¼ˆé€šç”¨å¸¸è¯†ï¼‰æ¥å›ç­”é—®é¢˜ã€‚
                2. **å¿…é¡»ä¸”åªèƒ½**åŸºäºä¸‹æ–¹çš„ã€å‚è€ƒèµ„æ–™ã€‘è¿›è¡Œå›ç­”ã€‚
                3. å¦‚æœã€å‚è€ƒèµ„æ–™ã€‘ä¸­æ²¡æœ‰åŒ…å«é—®é¢˜çš„ç­”æ¡ˆï¼Œè¯·ç›´æ¥å›å¤ï¼šâ€œæŠ±æ­‰ï¼Œæˆ‘çš„æœ¬åœ°çŸ¥è¯†åº“ä¸­æš‚æ—¶æ²¡æœ‰å…³äºè¿™ä¸ªé—®é¢˜çš„è®°å½•ã€‚â€ï¼Œä¸è¦ç¼–é€ ã€‚
                    {context}

                    å®¶é•¿çš„é—®é¢˜ï¼š{question}

                    è¯·æ¸©æŸ”ã€ä¸“ä¸šåœ°å›ç­”ã€‚å¦‚æœèµ„æ–™é‡Œæ²¡æœ‰ï¼Œè¯·ç›´è¯´ä¸çŸ¥é“ã€‚
                """
        prompt = ChatPromptTemplate.from_template(template)

        def format_docs(docs):
            return "\n\n".join(d.page_content for d in docs)

        chain = (
            {"context":retriever | format_docs,"question":RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )
        return chain.invoke(question)


#åˆ›å»ºä¸€ä¸ªå…¨å±€æ¡ˆä¾‹ï¼Œæ–¹ä¾¿main.pyè°ƒç”¨
ai_engine = AllergyAgentAI()